{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport bz2\nimport gc\nimport chardet\nimport re\nimport os\nimport pickle\nprint(os.listdir(\"../input\"))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model, Sequential\nfrom keras.layers import Dense, Embedding, Input, Conv1D, GlobalMaxPool1D, Dropout, concatenate, Layer, InputSpec\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import backend as K\nfrom keras import activations, initializers, regularizers, constraints\nfrom keras.utils.conv_utils import conv_output_length\nfrom keras.regularizers import l2\nfrom keras.constraints import maxnorm\nfrom keras.models import model_from_json\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_file = bz2.BZ2File('../input/amazonreviews/train.ft.txt.bz2')\ntest_file = bz2.BZ2File('../input/amazonreviews/test.ft.txt.bz2')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_file_lines = train_file.readlines()\ntest_file_lines = test_file.readlines()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_file, test_file","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_file_lines = [x.decode('utf-8') for x in train_file_lines]\ntest_file_lines = [x.decode('utf-8') for x in test_file_lines]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file_lines]\ntrain_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train_file_lines]\n\nfor i in range(len(train_sentences)):\n    train_sentences[i] = re.sub('\\d','0',train_sentences[i])\n    \ntest_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_file_lines]\ntest_sentences = [x.split(' ', 1)[1][:-1].lower() for x in test_file_lines]\n\nfor i in range(len(test_sentences)):\n    test_sentences[i] = re.sub('\\d','0',test_sentences[i])\n                                                       \nfor i in range(len(train_sentences)):\n    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n        \nfor i in range(len(test_sentences)):\n    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_file_lines, test_file_lines\ntrain_sentences[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Logistic Regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer(binary = True)\ncv.fit(train_sentences)\nlog_train=cv.transform(train_sentences)\nlog_test =cv.transform(test_sentences)\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#for c in [0.01, 0.05, 0.025, 0.5, 1]:\nlr = LogisticRegression(C=0.05)\nlr.fit(log_train, train_labels)\n# save the model to disk\nfilename = 'finalized_model.sav'\npickle.dump(lr, open(filename, 'wb'))\n \n# some time later...","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_model = pickle.load(open(\"../input/amazonreviews/test.ft.txt.bz2\", 'rb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The product is  good: RESULT:\")\ntesting = \"the product is  good\"\ntesting = [testing]\nprint(lr_model.predict(cv.transform(testing)))\n\nprint(\"The product is not good: RESULT:\")\ntesting = \"The product is not good\"\ntesting = [testing]\n\nprint(lr_model.predict(cv.transform(testing)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc=(accuracy_score(test_labels, lr_model.predict(log_test)))\nprint(\"Accuracy using LogisticRegression is\", acc)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}